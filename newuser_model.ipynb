{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "此模型针对新用户8到14天以及84到90天之内是否会活动，输入的特征为新用户在前7天的表现，得出的结果为该用户在8到14天是否有活动（0和1，用概率表示，1表示有活动）\n",
    "输入特征（参见newuser_needs.csv)：\n",
    "--输入特征根据前七天新用户的表现得到，以其中第二天为例子，剩下六天与之相同：\n",
    "\n",
    "1. 第2天在JollyChic平台上停留的时间\n",
    "2. 第2天浏览页面数\n",
    "3. 第2天是否注册\n",
    "4. 第2天加购次数\n",
    "5. 第2天搜索次数\n",
    "6. 第2天被埋点track到的次数\n",
    "7. 第2天访问了多少次详情页（单个产品的详情页多次也算多次）\n",
    "8. 第2天访问了多少产品，以详情页为准（同一产品的多次访问算一次）\n",
    "9. 第2天访问的产品，涉及多少个大类\n",
    "10. 第2天下了多少单\n",
    "11. 第2天订单总额\n",
    "\n",
    "--模型的预测标签为：\n",
    "\n",
    "1. purchased_in_8_14---第8到14天内有下单：1为有下单，0为没有\n",
    "2. active_in_8_14---第2周如果依旧有任何活动，都为1，否则为0\n",
    "3. activity_half_reduced_8_14---第二周内被埋点track到的活动减半\n",
    "4. purchased_in_8_90---第8到90天内有下单：1为有下单，0为没有\n",
    "5. active_in_84_90---90天内最后一周仍有活动，为1，否则为0\n",
    "\n",
    "注：搭建好的模型之后由于业务考虑只用来预测active_in_8_14。其他标签不用预测\n",
    "=================================================================================\n",
    "\n",
    "\n",
    "模型构建大体遵循：\n",
    "输入数据--随机抽样--训练特征--调优参数--评估预测结果--输入新数据进行预测\n",
    "\n",
    "=================================================================================\n",
    "'''\n",
    "'''\n",
    "导入必要的library\n",
    "'''\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/nick/newuser')\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "指定数据的路径并输入数据\n",
    "'''\n",
    "Location=r'C:/intern/new_user_prediction/newuser_model_sampled/0713_data_sampled.csv'\n",
    "df = pd.read_csv(Location,header=0,low_memory=False,delimiter='\\t')\n",
    "df_platform=df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "新建df_platform并将android设为0，ios设为1\n",
    "'''\n",
    "df_platform.loc[df_platform['platform']=='android','platform']=0\n",
    "df_platform.loc[df_platform['platform']=='ios','platform']=1\n",
    "'''\n",
    "新建备份，备份前17列的数据\n",
    "'''\n",
    "backup=df.iloc[:,0:17].copy()\n",
    "\n",
    "'''\n",
    "训练数据只保留第18列之后的\n",
    "'''\n",
    "df1=df.copy()\n",
    "df1=df1.drop(df.columns[0:17],axis=1)\n",
    "\n",
    "'''\n",
    "由于不需要预测activity_half_reduced_8_14，所以直接让它等于active_in_8_14，如果需要预测activity_half_reduced_8_14，注释下面一行\n",
    "'''\n",
    "df1['activity_half_reduced_8_14'] = df1['active_in_8_14']\n",
    "\n",
    "'''\n",
    "将标签0和1互换，1表示8到14天活动没有减半，0表示8到14天活动减半\n",
    "'''\n",
    "df1.loc[df1['activity_half_reduced_8_14']==0, 'activity_half_reduced_8_14']=2\n",
    "df1.loc[df1['activity_half_reduced_8_14']==1, 'activity_half_reduced_8_14']=0\n",
    "df1.loc[df1['activity_half_reduced_8_14']==2, 'activity_half_reduced_8_14']=1\n",
    "\n",
    "'''\n",
    "插入新建的df_platform，让手机平台也作为其中一个特征\n",
    "'''\n",
    "df1.insert(loc=0, column='platform', value=df_platform['platform'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andriod:2388\n",
      "ios:2611\n",
      "\n",
      "andriod=> 1:669\n",
      "andriod=> 0:1719\n",
      "ios=> 1:979\n",
      "ios=> 0:1632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "打印用户在两个手机型号上的数量分布\n",
    "'''\n",
    "print('andriod:{}'.format(len(backup[backup['platform']=='android'])))\n",
    "print('ios:{}'.format(len(backup[backup['platform']=='ios'])))\n",
    "print()\n",
    "print('andriod=> 1:{}'.format(len(df1[(df1['platform']==0) & (df1['active_in_8_14']==1)])))\n",
    "print('andriod=> 0:{}'.format(len(df1[(df1['platform']==0) & (df1['active_in_8_14']==0)])))\n",
    "\n",
    "print('ios=> 1:{}'.format(len(df1[(df1['platform']==1) & (df1['active_in_8_14']==1)])))\n",
    "print('ios=> 0:{}'.format(len(df1[(df1['platform']==1) & (df1['active_in_8_14']==0)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "feature_result_split: 此函数用于分离特征和标签（注意标签有三个），data为上面得到的原始数据，df_num表示标签的种类\n",
    "df_num=1: 表示active_in_8_14(第-4列)\n",
    "df_num=2: 表示active_in_84_90（第-1列）\n",
    "df_num=3: 表示activity_half_reduced_8_14（第-3列）\n",
    "'''\n",
    "def feature_result_split(data, df_num):\n",
    "    '''\n",
    "    把后边5列用作标签的列过滤掉，feature为生成的特征\n",
    "    '''\n",
    "    feature=data.iloc[:,:-5]\n",
    "\n",
    "    '''\n",
    "    提取三个标签单独成三列\n",
    "    '''\n",
    "    if df_num==1:\n",
    "        result=data.iloc[:,-4]\n",
    "\n",
    "\n",
    "    if df_num==2:\n",
    "        result=data.iloc[:,-1]\n",
    "\n",
    "\n",
    "    if df_num==3:\n",
    "        result=data.iloc[:,-3]\n",
    "\n",
    "\n",
    "    print('df:',df_num, '1 vs. 0:',len(result[result==1]), len(result[result==0]))\n",
    "\n",
    "    '''\n",
    "    输出生成的特征和标签\n",
    "    '''\n",
    "    return feature, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 1 1 vs. 0: 1648 3351\n",
      "df: 2 1 vs. 0: 41 4958\n",
      "df: 3 1 vs. 0: 3351 1648\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#feature and result split\n",
    "'''\n",
    "调用feature_result_split函数进行标签-特征分离\n",
    "'''\n",
    "X_1, Y_1 = feature_result_split(df1, 1)\n",
    "X_2, Y_2 = feature_result_split(df1, 2)\n",
    "X_3, Y_3 = feature_result_split(df1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "train_test_smote: 对输入的特征和标签做随机采样划分，feature_set为特征，result_set为标签，backup为之前备份的\n",
    "此函数将输入的数据随机划分为三大类：用于训练的数据，用于调试模型的数据，用于做模型表现评估的数据\n",
    "'''\n",
    "def train_test_smote(feature_set, result_set, backup):\n",
    "\n",
    "    smt=SMOTE(ratio='auto', random_state=10, k=None, k_neighbors=5, m=None, m_neighbors=10, out_step=0.5, kind='regular', svm_estimator=None, n_jobs=-1)\n",
    "\n",
    "    #do the train test data split randomly\n",
    "\n",
    "    '''\n",
    "    对特征和标签做SMOTE采样（Synthetic Minority Over-sampling Technique），目的是让不平衡的数据（标签为0的数据大于标签为1的数据）平衡\n",
    "    这里采用K nearest neighbors 的方法\n",
    "    注：SMOTE只能针对训练数据做数据扩充，否则会造成Data Leakage\n",
    "    这里，60%数据用来训练模型\n",
    "    20%的数据用来调试模型\n",
    "    20%的数据用来评估\n",
    "    '''\n",
    "    subset=np.floor(len(feature_set)*0.6).astype('int')\n",
    "\n",
    "    '''\n",
    "    training_f:训练用的特征\n",
    "    training_r:训练用的标签\n",
    "    validate_f:调模型用的特征\n",
    "    validate_r:调模型用的标签\n",
    "    test_f:评估用的特征\n",
    "    test_r:评估用的标签\n",
    "    '''\n",
    "    np.random.seed(1)\n",
    "    index_whole=np.random.choice(feature_set.index,subset,replace=False)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    index_validate=np.random.choice(pd.Index(index_whole), np.floor(subset/2).astype('int'), replace=False)\n",
    "\n",
    "    index_test=pd.Index(index_whole).difference(pd.Index(index_validate))\n",
    "\n",
    "    index_train=feature_set.index.difference(pd.Index(index_whole))\n",
    "\n",
    "    training_f=feature_set.loc[index_train,:]\n",
    "    training_r=result_set.loc[index_train]\n",
    "\n",
    "    validate_f=feature_set.loc[index_validate,:]\n",
    "    validate_r=result_set.loc[index_validate]\n",
    "    backup_validate=backup.loc[index_validate,:]\n",
    "\n",
    "    test_f=feature_set.loc[index_test,:]\n",
    "    test_r=result_set.loc[index_test]\n",
    "    backup_test=backup.loc[index_test,:]\n",
    "\n",
    "\n",
    "    #SMOTE\n",
    "    '''\n",
    "    只针对之前划分出的训练数据做SMOTE，training_f和training_r做完采样后生成training_f_af和training_r_af\n",
    "    '''\n",
    "    training_f_af, training_r_af=smt.fit_sample(training_f, training_r)\n",
    "\n",
    "    '''\n",
    "    打印做过SMOTE的训练数据和未做过SMOTE的剩余数据\n",
    "    '''\n",
    "    print('balanced data ratio in training set:')\n",
    "    print(len(training_r_af[training_r_af==0]) / len(training_r_af[training_r_af==1]))\n",
    "    print('balanced data ratio in test set:')\n",
    "    print(len(validate_r[validate_r==0]) / len(validate_r[validate_r==1]))\n",
    "\n",
    "    return training_f_af, training_r_af, validate_f, validate_r, backup_validate, test_f, test_r, backup_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced data ratio in training set:\n",
      "1.0\n",
      "balanced data ratio in test set:\n",
      "2.090721649484536\n",
      "balanced data ratio in training set:\n",
      "1.0\n",
      "balanced data ratio in test set:\n",
      "123.91666666666667\n",
      "balanced data ratio in training set:\n",
      "1.0\n",
      "balanced data ratio in test set:\n",
      "0.47830374753451677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "针对三个标签分别三次调用train_test_smote函数进行采样\n",
    "'''\n",
    "X1_train, Y1_train, X1_validate, Y1_validate, backup1_validate, X1_test, Y1_test, backup1_test = train_test_smote(X_1, Y_1, backup)\n",
    "X2_train, Y2_train, X2_validate, Y2_validate, backup2_validate, X2_test, Y2_test, backup2_test = train_test_smote(X_2, Y_2, backup)\n",
    "X3_train, Y3_train, X3_validate, Y3_validate, backup3_validate, X3_test, Y3_test, backup3_test = train_test_smote(X_3, Y_3, backup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "train: 用来训练模型的函数 （这里用random forest)\n",
    "X_train: 训练数据的特征\n",
    "X_test: 调模型用的数据的特征\n",
    "Y_train: 训练数据的标签\n",
    "Y_test: 调模型用的数据的标签\n",
    "backup_test: 调模型用的数据对应的备份特征\n",
    "name: 标签的名称\n",
    "'''\n",
    "\n",
    "def train(X_train, X_test, Y_train, Y_test, backup_test,name):\n",
    "    print('Training models...')\n",
    "    best_model = None\n",
    "    best_ratio, best_ne, best_md = 0, 0, 0\n",
    "    #for ne in np.arange(30,500,50):\n",
    "    #    for md in np.arange(10,200,50):\n",
    "    for ne in [100]:\n",
    "        for md in [10]:\n",
    "\n",
    "            rf = RandomForestClassifier(max_depth=md, n_estimators=ne,n_jobs=-1, random_state = 10)\n",
    "            rf_fit=rf.fit(X_train, Y_train)\n",
    "            prediction=rf_fit.predict(X_test)\n",
    "            prediction_pro=rf_fit.predict_proba(X_test)\n",
    "            '''\n",
    "            用ROC_AUC作为指标调参数\n",
    "            '''\n",
    "            roc = roc_auc_score(Y_test, prediction_pro[:,1])\n",
    "            '''\n",
    "            也可以0.7的概率以上的准确率去调参，这里没有用这个方法\n",
    "            '''\n",
    "            #acc = accuracy_score(Y_test,prediction)\n",
    "            #temp=pd.DataFrame()\n",
    "\n",
    "            #temp['Y_test']=Y_test\n",
    "            #temp['Y_predict']=prediction\n",
    "            #temp['prob']=prediction_pro[:,1]\n",
    "            #temp=temp.sort_values(by='prob',ascending=False)\n",
    "            #temp=temp[temp['prob']>=0.7]\n",
    "\n",
    "            #count=len(temp[(temp['Y_test']==1) & (temp['Y_predict']==1)])\n",
    "            #ratio=count / len(temp)\n",
    "            ratio=roc\n",
    "            print('-----------------------------------------------------------')\n",
    "            if ratio > best_ratio:\n",
    "                best_md = md\n",
    "                best_ne = ne\n",
    "                best_ratio = ratio\n",
    "                best_model = rf_fit\n",
    "            '''\n",
    "            打印每个模型对应的参数和confusion matrix\n",
    "            '''\n",
    "            print('n_estimators:  max_depth:',(ne, md))\n",
    "            print('confusion matrix\\n', confusion_matrix(Y_test,prediction))\n",
    "            print('ratio:{0}'.format(ratio))\n",
    "            #print('roc_auc',roc_auc_score(Y_test, prediction_pro[:,1]))\n",
    "            #print('accuracy',accuracy_score(Y_test,prediction))\n",
    "            print()\n",
    "            print()\n",
    "    '''\n",
    "    打印最佳模型对应的参数和confusion matrix\n",
    "    '''\n",
    "    print('best_n:{0}, best_depth:{1}'.format(best_ne, best_md))\n",
    "    print('best_ratio using validation set:{0}'.format(best_ratio))\n",
    "\n",
    "    '''\n",
    "    输出最佳模型（共三个模型，名称被储存在name变量里）\n",
    "    '''\n",
    "    joblib.dump(best_model, 'newuser_best_model_{}_0715.pkl'.format(name))\n",
    "\n",
    "    print('best_model written to file!')\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "evaluate: 用来评估模型的函数 （这里用random forest)\n",
    "X_test: 评估数据的特征\n",
    "Y_test: 评估数据的标签\n",
    "backup_test: 评估数据对应的备份特征\n",
    "'''\n",
    "def evaluate(model, X_test, Y_test, backup_test):\n",
    "    print('Evalluating models...')\n",
    "    prediction_pro=model.predict_proba(X_test)\n",
    "    prediction=model.predict(X_test)\n",
    "    '''\n",
    "    得到ROC_AUC score\n",
    "    '''\n",
    "    roc = roc_auc_score(Y_test, prediction_pro[:,1])\n",
    "    ratio=roc\n",
    "\n",
    "    #temp=backup_test.copy()\n",
    "    #temp['Y_test']=Y_test\n",
    "    #temp['Y_predict']=prediction\n",
    "    #temp['prob']=prediction_pro[:,1]\n",
    "    #temp=temp.sort_values(by='prob',ascending=False)\n",
    "    #temp=temp[temp['prob']>=0.7]\n",
    "    #count=len(temp[(temp['Y_test']==1) & (temp['Y_predict']==1)])\n",
    "    #ratio=count / len(temp)\n",
    "    print('The ratio using evaluation set:{0}'.format(ratio))\n",
    "\n",
    "    outfile=pd. DataFrame()\n",
    "    outfile=backup_test.copy()\n",
    "    outfile['label']=Y_test\n",
    "    outfile['prob']=prediction_pro[:,1]\n",
    "\n",
    "    '''\n",
    "    打印用户在两个不同平台（android和ios)的准确率\n",
    "\n",
    "    准确率计算方法：\n",
    "    1. 生成临时dataframe存储当输入用于调试模型的数据后模型给出的结果 （真实标签，模型预测的标签，模型预测的概率）\n",
    "    2. 把生成的dataframe按照模型预测的概率按从大到小排列\n",
    "    3. 选取概率0.64作为门槛\n",
    "    4. 统计概率大于等于0.64的这些数据中模型预测正确的数量 N\n",
    "    5. M为概率大于等于0.64对应的所有用户，N/M为模型的准确率\n",
    "\n",
    "    '''\n",
    "\n",
    "    #print('andriod=> 1:{}'.format(len(outfile[(outfile['platform']=='android') & (outfile['label']==1) & (outfile['prob']>=0.64)])  /  len(outfile[(outfile['platform']=='android')  & (outfile['prob']>=0.64)])))\n",
    "\n",
    "    #print('ios=> 1:{}'.format(len(outfile[(outfile['platform']=='ios') & (outfile['label']==1) & (outfile['prob']>=0.64)])  /  len(outfile[(outfile['platform']=='ios')  & (outfile['prob']>=0.64)])))\n",
    "\n",
    "\n",
    "    print('Evaluate Done!')\n",
    "    return outfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "-----------------------------------------------------------\n",
      "n_estimators:  max_depth: (100, 10)\n",
      "confusion matrix\n",
      " [[868 146]\n",
      " [196 289]]\n",
      "ratio:0.7979096768946095\n",
      "\n",
      "\n",
      "best_n:100, best_depth:10\n",
      "best_ratio using validation set:0.7979096768946095\n",
      "best_model written to file!\n",
      "Training models...\n",
      "-----------------------------------------------------------\n",
      "n_estimators:  max_depth: (100, 10)\n",
      "confusion matrix\n",
      " [[1474   13]\n",
      " [  12    0]]\n",
      "ratio:0.6978536202645146\n",
      "\n",
      "\n",
      "best_n:100, best_depth:10\n",
      "best_ratio using validation set:0.6978536202645146\n",
      "best_model written to file!\n",
      "Training models...\n",
      "-----------------------------------------------------------\n",
      "n_estimators:  max_depth: (100, 10)\n",
      "confusion matrix\n",
      " [[289 196]\n",
      " [146 868]]\n",
      "ratio:0.7979096768946095\n",
      "\n",
      "\n",
      "best_n:100, best_depth:10\n",
      "best_ratio using validation set:0.7979096768946095\n",
      "best_model written to file!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "三次调用train函数训练模型\n",
    "'''\n",
    "\n",
    "model1 = train(X1_train, X1_validate, Y1_train, Y1_validate, backup1_validate,'active_in_8_14')\n",
    "model2 = train(X2_train, X2_validate, Y2_train, Y2_validate, backup2_validate,'active_in_84_90')\n",
    "model3 = train(X3_train, X3_validate, Y3_train, Y3_validate, backup3_validate,'activity_half_reduced_8_14')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalluating models...\n",
      "The ratio using evaluation set:0.8255334877531911\n",
      "Evaluate Done!\n",
      "Evalluating models...\n",
      "The ratio using evaluation set:0.6865771812080538\n",
      "Evaluate Done!\n",
      "Evalluating models...\n",
      "The ratio using evaluation set:0.825533487753191\n",
      "Evaluate Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "三次调用evaluate函数进行模型评估，得到三个dataframe\n",
    "'''\n",
    "\n",
    "\n",
    "out_file1 = evaluate(model1, X1_test, Y1_test, backup1_test)\n",
    "out_file2 = evaluate(model2, X2_test, Y2_test, backup2_test)\n",
    "out_file3 = evaluate(model3, X3_test, Y3_test, backup3_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation written to file!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "将三个评估得出的dataframe合并成一个dataframe,加上对应的标签名\n",
    "'''\n",
    "out_file1=out_file1.rename(columns={'label':'active_in_8_14_label','prob':'active_in_8_14_probability'})\n",
    "out_file2=out_file2.rename(columns={'label':'active_in_84_90_label','prob':'active_in_84_90_probability'})\n",
    "out_file3=out_file3.rename(columns={'label':'activity_half_reduced_8_14_label','prob':'activity_half_reduced_8_14_probability'})\n",
    "out_file=pd.DataFrame()\n",
    "out_file=pd.merge(out_file1, out_file2[['active_in_84_90_label','active_in_84_90_probability']],left_index=True, right_index=True)\n",
    "out_file=pd.merge(out_file, out_file3[['activity_half_reduced_8_14_label','activity_half_reduced_8_14_probability']],left_index=True, right_index=True)\n",
    "\n",
    "'''\n",
    "输出最终的dataframe\n",
    "'''\n",
    "path = 'C:/intern/new_user_prediction/newuser_model_sampled'\n",
    "out_file.to_csv(os.path.join(path,'0713_data_evaluation.csv'),index=False)\n",
    "print('Evaluation written to file!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "predict: 训练好的模型用于预测测试数据\n",
    "path: 输入原始数据的路径\n",
    "pklpath: 三个最佳模型PKL文件的路径\n",
    "model_name: 三个最佳模型PKL的名称\n",
    "'''\n",
    "def predict(path, pklpath, model_name):\n",
    "    print('Starting prediction...')\n",
    "    from sklearn.externals import joblib\n",
    "    df = pd.read_csv(path,header=0,low_memory=False,delimiter='\\t')\n",
    "\n",
    "\n",
    "    '''\n",
    "    重复最早处理原始数据的步骤，得到特征\n",
    "    '''\n",
    "    df_platform=df.copy()\n",
    "    df_platform.loc[df_platform['platform']=='android','platform']=0\n",
    "    df_platform.loc[df_platform['platform']=='ios','platform']=1\n",
    "\n",
    "\n",
    "    df1=df.copy()\n",
    "    df1=df1.drop(df.columns[0:17],axis=1)\n",
    "\n",
    "    df1.insert(loc=0, column='platform', value=df_platform['platform'])\n",
    "    df2=df.copy()\n",
    "\n",
    "    '''\n",
    "    备份原始数据中的指定列，需要保留这些列作为输出\n",
    "    '''\n",
    "    df_new=pd.DataFrame()\n",
    "    df_new['site_id']=df2['site_id']\n",
    "    df_new['device_id']=df2['device_id']\n",
    "    df_new['cookie_id']=df2['cookie_id']\n",
    "    df_new['data_installed']=df2['data_installed']\n",
    "    df_new['platform']=df2['platform']\n",
    "    df_new['country']=df2['country']\n",
    "\n",
    "    df_new['day1_prod_num']=df2['day1_prod_num']\n",
    "    df_new['day2_prod_num']=df2['day2_prod_num']\n",
    "    df_new['day3_prod_num']=df2['day3_prod_num']\n",
    "    df_new['day4_prod_num']=df2['day4_prod_num']\n",
    "    df_new['day5_prod_num']=df2['day5_prod_num']\n",
    "    df_new['day6_prod_num']=df2['day6_prod_num']\n",
    "    df_new['day7_prod_num']=df2['day7_prod_num']\n",
    "\n",
    "\n",
    "    feature=df1.iloc[:,:]\n",
    "\n",
    "    '''\n",
    "    根据指定PKL路径和名称读取最佳模型（共三个）\n",
    "    '''\n",
    "    pkl = open(os.path.join(pklpath,model_name),'rb')\n",
    "    model = joblib.load(pkl)\n",
    "\n",
    "    '''\n",
    "    做模型预测\n",
    "    '''\n",
    "    prediction = model.predict(feature)\n",
    "    prediction_pro = model.predict_proba(feature)\n",
    "\n",
    "    #feature importance\n",
    "\n",
    "    '''\n",
    "    计算测试数据的特征重要性\n",
    "    '''\n",
    "    featurename=feature.columns\n",
    "    importances=model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    fi = pd.DataFrame()\n",
    "    fi['feature_name'] = featurename[indices]\n",
    "    fi['importance'] = importances[indices]\n",
    "\n",
    "\n",
    "    print('Prediction complete!')\n",
    "\n",
    "    '''\n",
    "    返回模型预测的标签，模型预测的概率，备份的数据，和特征重要性\n",
    "    '''\n",
    "    return prediction, prediction_pro[:,1], df_new, fi\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction...\n",
      "Prediction complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "指定预测文件的路径和名称，调用predict函数做模型预测\n",
    "注：这里只需要预测active_in_8_14，其余两个标签不需要做预测，因此只调用predict函数一次\n",
    "'''\n",
    "\n",
    "path='C:/intern/new_user_prediction/newuser_model_sampled/0715_test_sampled.csv'\n",
    "prediction1, prediction1_proba, backup, fi1= predict(path,'C:/intern/new_user_prediction/newuser_model_sampled','newuser_best_model_active_in_8_14_0715.pkl')\n",
    "\n",
    "outfile=pd.DataFrame()\n",
    "outfile=backup\n",
    "outfile['active_in_8_14_probability'] = prediction1_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "在模型的输出中做A/B TEST，步骤为：\n",
    "新建两个dataframe, outfile_A和outfile_B\n",
    "如果cookie_id第八位数字在0到7，则把这部分用户数据放入A组\n",
    "如果cookie_id的八位不在0到7，则把这部分用户数据放入B组\n",
    "业务部门将会对A组做例如短信推送，广告，之类的\n",
    "B组将不做任何处理\n",
    "对比A B组未来一个星期的活动\n",
    "'''\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#for A B test\n",
    "index_A=[]\n",
    "index_B=[]\n",
    "for i in range(0,len(outfile)):\n",
    "    if outfile.iloc[i]['cookie_id'][7] not in ['0','1','2','3','4','5','6','7']:\n",
    "        index_B.append(i)\n",
    "    else:\n",
    "        index_A.append(i)\n",
    "\n",
    "outfile_A=outfile.loc[index_A,:]\n",
    "outfile_B=outfile.loc[index_B,:]\n",
    "\n",
    "\n",
    "outfile_A=outfile_A.sort_values(by='active_in_8_14_probability', ascending=False)\n",
    "outfile_B=outfile_B.sort_values(by='active_in_8_14_probability', ascending=False)\n",
    "outfile=outfile.sort_values(by='active_in_8_14_probability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file...\n",
      "Finish writing to file.\n",
      "Finish writing feature importances to file.\n",
      "All complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n注释结束\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "输出A组预测数据和B组预测数据\n",
    "'''\n",
    "\n",
    "print('Writing to file...')\n",
    "path = 'C:/intern/new_user_prediction/newuser_model_sampled'\n",
    "outfile_A.to_csv(os.path.join(path, '0715_test_A_prediction.csv'),index=False)\n",
    "outfile_B.to_csv(os.path.join(path, '0715_test_B_prediction.csv'),index=False)\n",
    "outfile.to_csv(os.path.join(path, '0715_test_prediction.csv'),index=False)\n",
    "print('Finish writing to file.')\n",
    "\n",
    "\n",
    "'''\n",
    "输出特征重要性\n",
    "'''\n",
    "fi = pd.DataFrame()\n",
    "fi['feature_active_in_8_14'] = fi1['feature_name']\n",
    "fi['importance_active_in_8_14'] = fi1['importance']\n",
    "\n",
    "\n",
    "fi.to_csv(os.path.join(path, '0715_test_prediction_feature_importances.csv'),index=False)\n",
    "print('Finish writing feature importances to file.')\n",
    "print('All complete!')\n",
    "\n",
    "'''\n",
    "注释结束\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
