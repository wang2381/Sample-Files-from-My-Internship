{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/nick/newuser')\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Location=r'C:/intern/new_user_prediction/newuser_model_sampled/0713_data_sampled.csv'\n",
    "df = pd.read_csv(Location,header=0,low_memory=False,delimiter='\\t')\n",
    "df_platform=df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_platform.loc[df_platform['platform']=='android','platform']=0\n",
    "df_platform.loc[df_platform['platform']=='ios','platform']=1\n",
    "\n",
    "backup=df.iloc[:,0:17].copy()\n",
    "\n",
    "\n",
    "df1=df.copy()\n",
    "df1=df1.drop(df.columns[0:17],axis=1)\n",
    "\n",
    "df1['activity_half_reduced_8_14'] = df1['active_in_8_14']\n",
    "\n",
    "\n",
    "df1.loc[df1['activity_half_reduced_8_14']==0, 'activity_half_reduced_8_14']=2\n",
    "df1.loc[df1['activity_half_reduced_8_14']==1, 'activity_half_reduced_8_14']=0\n",
    "df1.loc[df1['activity_half_reduced_8_14']==2, 'activity_half_reduced_8_14']=1\n",
    "\n",
    "df1.insert(loc=0, column='platform', value=df_platform['platform'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andriod:2388\n",
      "ios:2611\n",
      "\n",
      "andriod=> 1:669\n",
      "andriod=> 0:1719\n",
      "ios=> 1:979\n",
      "ios=> 0:1632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('andriod:{}'.format(len(backup[backup['platform']=='android'])))\n",
    "print('ios:{}'.format(len(backup[backup['platform']=='ios'])))\n",
    "print()\n",
    "print('andriod=> 1:{}'.format(len(df1[(df1['platform']==0) & (df1['active_in_8_14']==1)])))\n",
    "print('andriod=> 0:{}'.format(len(df1[(df1['platform']==0) & (df1['active_in_8_14']==0)])))\n",
    "\n",
    "print('ios=> 1:{}'.format(len(df1[(df1['platform']==1) & (df1['active_in_8_14']==1)])))\n",
    "print('ios=> 0:{}'.format(len(df1[(df1['platform']==1) & (df1['active_in_8_14']==0)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def feature_result_split(data, df_num):\n",
    "  \n",
    "    feature=data.iloc[:,:-5]\n",
    "\n",
    "    if df_num==1:\n",
    "        result=data.iloc[:,-4]\n",
    "\n",
    "\n",
    "    if df_num==2:\n",
    "        result=data.iloc[:,-1]\n",
    "\n",
    "\n",
    "    if df_num==3:\n",
    "        result=data.iloc[:,-3]\n",
    "\n",
    "\n",
    "    print('df:',df_num, '1 vs. 0:',len(result[result==1]), len(result[result==0]))\n",
    "\n",
    "  \n",
    "    return feature, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 1 1 vs. 0: 1648 3351\n",
      "df: 2 1 vs. 0: 41 4958\n",
      "df: 3 1 vs. 0: 3351 1648\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#feature and result split\n",
    "\n",
    "X_1, Y_1 = feature_result_split(df1, 1)\n",
    "X_2, Y_2 = feature_result_split(df1, 2)\n",
    "X_3, Y_3 = feature_result_split(df1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_test_smote(feature_set, result_set, backup):\n",
    "\n",
    "    smt=SMOTE(ratio='auto', random_state=10, k=None, k_neighbors=5, m=None, m_neighbors=10, out_step=0.5, kind='regular', svm_estimator=None, n_jobs=-1)\n",
    "\n",
    "    #do the train test data split randomly\n",
    "\n",
    "  \n",
    "    subset=np.floor(len(feature_set)*0.6).astype('int')\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "    index_whole=np.random.choice(feature_set.index,subset,replace=False)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    index_validate=np.random.choice(pd.Index(index_whole), np.floor(subset/2).astype('int'), replace=False)\n",
    "\n",
    "    index_test=pd.Index(index_whole).difference(pd.Index(index_validate))\n",
    "\n",
    "    index_train=feature_set.index.difference(pd.Index(index_whole))\n",
    "\n",
    "    training_f=feature_set.loc[index_train,:]\n",
    "    training_r=result_set.loc[index_train]\n",
    "\n",
    "    validate_f=feature_set.loc[index_validate,:]\n",
    "    validate_r=result_set.loc[index_validate]\n",
    "    backup_validate=backup.loc[index_validate,:]\n",
    "\n",
    "    test_f=feature_set.loc[index_test,:]\n",
    "    test_r=result_set.loc[index_test]\n",
    "    backup_test=backup.loc[index_test,:]\n",
    "\n",
    "\n",
    "    #SMOTE\n",
    "\n",
    "    training_f_af, training_r_af=smt.fit_sample(training_f, training_r)\n",
    "\n",
    "   \n",
    "    print('balanced data ratio in training set:')\n",
    "    print(len(training_r_af[training_r_af==0]) / len(training_r_af[training_r_af==1]))\n",
    "    print('balanced data ratio in test set:')\n",
    "    print(len(validate_r[validate_r==0]) / len(validate_r[validate_r==1]))\n",
    "\n",
    "    return training_f_af, training_r_af, validate_f, validate_r, backup_validate, test_f, test_r, backup_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced data ratio in training set:\n",
      "1.0\n",
      "balanced data ratio in test set:\n",
      "2.090721649484536\n",
      "balanced data ratio in training set:\n",
      "1.0\n",
      "balanced data ratio in test set:\n",
      "123.91666666666667\n",
      "balanced data ratio in training set:\n",
      "1.0\n",
      "balanced data ratio in test set:\n",
      "0.47830374753451677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X1_train, Y1_train, X1_validate, Y1_validate, backup1_validate, X1_test, Y1_test, backup1_test = train_test_smote(X_1, Y_1, backup)\n",
    "X2_train, Y2_train, X2_validate, Y2_validate, backup2_validate, X2_test, Y2_test, backup2_test = train_test_smote(X_2, Y_2, backup)\n",
    "X3_train, Y3_train, X3_validate, Y3_validate, backup3_validate, X3_test, Y3_test, backup3_test = train_test_smote(X_3, Y_3, backup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(X_train, X_test, Y_train, Y_test, backup_test,name):\n",
    "    print('Training models...')\n",
    "    best_model = None\n",
    "    best_ratio, best_ne, best_md = 0, 0, 0\n",
    "    #for ne in np.arange(30,500,50):\n",
    "    #    for md in np.arange(10,200,50):\n",
    "    for ne in [100]:\n",
    "        for md in [10]:\n",
    "\n",
    "            rf = RandomForestClassifier(max_depth=md, n_estimators=ne,n_jobs=-1, random_state = 10)\n",
    "            rf_fit=rf.fit(X_train, Y_train)\n",
    "            prediction=rf_fit.predict(X_test)\n",
    "            prediction_pro=rf_fit.predict_proba(X_test)\n",
    "          \n",
    "            roc = roc_auc_score(Y_test, prediction_pro[:,1])\n",
    "           \n",
    "            #acc = accuracy_score(Y_test,prediction)\n",
    "            #temp=pd.DataFrame()\n",
    "\n",
    "            #temp['Y_test']=Y_test\n",
    "            #temp['Y_predict']=prediction\n",
    "            #temp['prob']=prediction_pro[:,1]\n",
    "            #temp=temp.sort_values(by='prob',ascending=False)\n",
    "            #temp=temp[temp['prob']>=0.7]\n",
    "\n",
    "            #count=len(temp[(temp['Y_test']==1) & (temp['Y_predict']==1)])\n",
    "            #ratio=count / len(temp)\n",
    "            ratio=roc\n",
    "            print('-----------------------------------------------------------')\n",
    "            if ratio > best_ratio:\n",
    "                best_md = md\n",
    "                best_ne = ne\n",
    "                best_ratio = ratio\n",
    "                best_model = rf_fit\n",
    "            \n",
    "            print('n_estimators:  max_depth:',(ne, md))\n",
    "            print('confusion matrix\\n', confusion_matrix(Y_test,prediction))\n",
    "            print('ratio:{0}'.format(ratio))\n",
    "            #print('roc_auc',roc_auc_score(Y_test, prediction_pro[:,1]))\n",
    "            #print('accuracy',accuracy_score(Y_test,prediction))\n",
    "            print()\n",
    "            print()\n",
    "   \n",
    "    print('best_n:{0}, best_depth:{1}'.format(best_ne, best_md))\n",
    "    print('best_ratio using validation set:{0}'.format(best_ratio))\n",
    "\n",
    "    \n",
    "    joblib.dump(best_model, 'newuser_best_model_{}_0715.pkl'.format(name))\n",
    "\n",
    "    print('best_model written to file!')\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, X_test, Y_test, backup_test):\n",
    "    print('Evalluating models...')\n",
    "    prediction_pro=model.predict_proba(X_test)\n",
    "    prediction=model.predict(X_test)\n",
    "  \n",
    "    roc = roc_auc_score(Y_test, prediction_pro[:,1])\n",
    "    ratio=roc\n",
    "\n",
    "    #temp=backup_test.copy()\n",
    "    #temp['Y_test']=Y_test\n",
    "    #temp['Y_predict']=prediction\n",
    "    #temp['prob']=prediction_pro[:,1]\n",
    "    #temp=temp.sort_values(by='prob',ascending=False)\n",
    "    #temp=temp[temp['prob']>=0.7]\n",
    "    #count=len(temp[(temp['Y_test']==1) & (temp['Y_predict']==1)])\n",
    "    #ratio=count / len(temp)\n",
    "    print('The ratio using evaluation set:{0}'.format(ratio))\n",
    "\n",
    "    outfile=pd. DataFrame()\n",
    "    outfile=backup_test.copy()\n",
    "    outfile['label']=Y_test\n",
    "    outfile['prob']=prediction_pro[:,1]\n",
    "\n",
    "\n",
    "    #print('andriod=> 1:{}'.format(len(outfile[(outfile['platform']=='android') & (outfile['label']==1) & (outfile['prob']>=0.64)])  /  len(outfile[(outfile['platform']=='android')  & (outfile['prob']>=0.64)])))\n",
    "\n",
    "    #print('ios=> 1:{}'.format(len(outfile[(outfile['platform']=='ios') & (outfile['label']==1) & (outfile['prob']>=0.64)])  /  len(outfile[(outfile['platform']=='ios')  & (outfile['prob']>=0.64)])))\n",
    "\n",
    "\n",
    "    print('Evaluate Done!')\n",
    "    return outfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "-----------------------------------------------------------\n",
      "n_estimators:  max_depth: (100, 10)\n",
      "confusion matrix\n",
      " [[868 146]\n",
      " [196 289]]\n",
      "ratio:0.7979096768946095\n",
      "\n",
      "\n",
      "best_n:100, best_depth:10\n",
      "best_ratio using validation set:0.7979096768946095\n",
      "best_model written to file!\n",
      "Training models...\n",
      "-----------------------------------------------------------\n",
      "n_estimators:  max_depth: (100, 10)\n",
      "confusion matrix\n",
      " [[1474   13]\n",
      " [  12    0]]\n",
      "ratio:0.6978536202645146\n",
      "\n",
      "\n",
      "best_n:100, best_depth:10\n",
      "best_ratio using validation set:0.6978536202645146\n",
      "best_model written to file!\n",
      "Training models...\n",
      "-----------------------------------------------------------\n",
      "n_estimators:  max_depth: (100, 10)\n",
      "confusion matrix\n",
      " [[289 196]\n",
      " [146 868]]\n",
      "ratio:0.7979096768946095\n",
      "\n",
      "\n",
      "best_n:100, best_depth:10\n",
      "best_ratio using validation set:0.7979096768946095\n",
      "best_model written to file!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model1 = train(X1_train, X1_validate, Y1_train, Y1_validate, backup1_validate,'active_in_8_14')\n",
    "model2 = train(X2_train, X2_validate, Y2_train, Y2_validate, backup2_validate,'active_in_84_90')\n",
    "model3 = train(X3_train, X3_validate, Y3_train, Y3_validate, backup3_validate,'activity_half_reduced_8_14')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalluating models...\n",
      "The ratio using evaluation set:0.8255334877531911\n",
      "Evaluate Done!\n",
      "Evalluating models...\n",
      "The ratio using evaluation set:0.6865771812080538\n",
      "Evaluate Done!\n",
      "Evalluating models...\n",
      "The ratio using evaluation set:0.825533487753191\n",
      "Evaluate Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "out_file1 = evaluate(model1, X1_test, Y1_test, backup1_test)\n",
    "out_file2 = evaluate(model2, X2_test, Y2_test, backup2_test)\n",
    "out_file3 = evaluate(model3, X3_test, Y3_test, backup3_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation written to file!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out_file1=out_file1.rename(columns={'label':'active_in_8_14_label','prob':'active_in_8_14_probability'})\n",
    "out_file2=out_file2.rename(columns={'label':'active_in_84_90_label','prob':'active_in_84_90_probability'})\n",
    "out_file3=out_file3.rename(columns={'label':'activity_half_reduced_8_14_label','prob':'activity_half_reduced_8_14_probability'})\n",
    "out_file=pd.DataFrame()\n",
    "out_file=pd.merge(out_file1, out_file2[['active_in_84_90_label','active_in_84_90_probability']],left_index=True, right_index=True)\n",
    "out_file=pd.merge(out_file, out_file3[['activity_half_reduced_8_14_label','activity_half_reduced_8_14_probability']],left_index=True, right_index=True)\n",
    "\n",
    "path = 'C:/intern/new_user_prediction/newuser_model_sampled'\n",
    "out_file.to_csv(os.path.join(path,'0713_data_evaluation.csv'),index=False)\n",
    "print('Evaluation written to file!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(path, pklpath, model_name):\n",
    "    print('Starting prediction...')\n",
    "    from sklearn.externals import joblib\n",
    "    df = pd.read_csv(path,header=0,low_memory=False,delimiter='\\t')\n",
    "\n",
    "\n",
    "  \n",
    "    df_platform=df.copy()\n",
    "    df_platform.loc[df_platform['platform']=='android','platform']=0\n",
    "    df_platform.loc[df_platform['platform']=='ios','platform']=1\n",
    "\n",
    "\n",
    "    df1=df.copy()\n",
    "    df1=df1.drop(df.columns[0:17],axis=1)\n",
    "\n",
    "    df1.insert(loc=0, column='platform', value=df_platform['platform'])\n",
    "    df2=df.copy()\n",
    "\n",
    "    \n",
    "    df_new=pd.DataFrame()\n",
    "    df_new['site_id']=df2['site_id']\n",
    "    df_new['device_id']=df2['device_id']\n",
    "    df_new['cookie_id']=df2['cookie_id']\n",
    "    df_new['data_installed']=df2['data_installed']\n",
    "    df_new['platform']=df2['platform']\n",
    "    df_new['country']=df2['country']\n",
    "\n",
    "    df_new['day1_prod_num']=df2['day1_prod_num']\n",
    "    df_new['day2_prod_num']=df2['day2_prod_num']\n",
    "    df_new['day3_prod_num']=df2['day3_prod_num']\n",
    "    df_new['day4_prod_num']=df2['day4_prod_num']\n",
    "    df_new['day5_prod_num']=df2['day5_prod_num']\n",
    "    df_new['day6_prod_num']=df2['day6_prod_num']\n",
    "    df_new['day7_prod_num']=df2['day7_prod_num']\n",
    "\n",
    "\n",
    "    feature=df1.iloc[:,:]\n",
    "\n",
    "  \n",
    "    pkl = open(os.path.join(pklpath,model_name),'rb')\n",
    "    model = joblib.load(pkl)\n",
    "\n",
    "    prediction = model.predict(feature)\n",
    "    prediction_pro = model.predict_proba(feature)\n",
    "\n",
    "    #feature importance\n",
    "\n",
    "    \n",
    "    featurename=feature.columns\n",
    "    importances=model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    fi = pd.DataFrame()\n",
    "    fi['feature_name'] = featurename[indices]\n",
    "    fi['importance'] = importances[indices]\n",
    "\n",
    "\n",
    "    print('Prediction complete!')\n",
    "\n",
    "  \n",
    "    return prediction, prediction_pro[:,1], df_new, fi\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction...\n",
      "Prediction complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path='C:/intern/new_user_prediction/newuser_model_sampled/0715_test_sampled.csv'\n",
    "prediction1, prediction1_proba, backup, fi1= predict(path,'C:/intern/new_user_prediction/newuser_model_sampled','newuser_best_model_active_in_8_14_0715.pkl')\n",
    "\n",
    "outfile=pd.DataFrame()\n",
    "outfile=backup\n",
    "outfile['active_in_8_14_probability'] = prediction1_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#for A B test\n",
    "index_A=[]\n",
    "index_B=[]\n",
    "for i in range(0,len(outfile)):\n",
    "    if outfile.iloc[i]['cookie_id'][7] not in ['0','1','2','3','4','5','6','7']:\n",
    "        index_B.append(i)\n",
    "    else:\n",
    "        index_A.append(i)\n",
    "\n",
    "outfile_A=outfile.loc[index_A,:]\n",
    "outfile_B=outfile.loc[index_B,:]\n",
    "\n",
    "\n",
    "outfile_A=outfile_A.sort_values(by='active_in_8_14_probability', ascending=False)\n",
    "outfile_B=outfile_B.sort_values(by='active_in_8_14_probability', ascending=False)\n",
    "outfile=outfile.sort_values(by='active_in_8_14_probability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file...\n",
      "Finish writing to file.\n",
      "Finish writing feature importances to file.\n",
      "All complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n注释结束\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Writing to file...')\n",
    "path = 'C:/intern/new_user_prediction/newuser_model_sampled'\n",
    "outfile_A.to_csv(os.path.join(path, '0715_test_A_prediction.csv'),index=False)\n",
    "outfile_B.to_csv(os.path.join(path, '0715_test_B_prediction.csv'),index=False)\n",
    "outfile.to_csv(os.path.join(path, '0715_test_prediction.csv'),index=False)\n",
    "print('Finish writing to file.')\n",
    "\n",
    "\n",
    "fi = pd.DataFrame()\n",
    "fi['feature_active_in_8_14'] = fi1['feature_name']\n",
    "fi['importance_active_in_8_14'] = fi1['importance']\n",
    "\n",
    "\n",
    "fi.to_csv(os.path.join(path, '0715_test_prediction_feature_importances.csv'),index=False)\n",
    "print('Finish writing feature importances to file.')\n",
    "print('All complete!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
